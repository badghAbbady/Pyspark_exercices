{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "[PATH_NOT_FOUND] Path does not exist: file:/Users/badrelabbady/Downloads/ventes2.csv.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/badrelabbady/Downloads/modele-architec_exoPyspark.ipynb Cellule 1\u001b[0m line \u001b[0;36m1\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/badrelabbady/Downloads/modele-architec_exoPyspark.ipynb#W4sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mpyspark\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39msql\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mfunctions\u001b[39;00m \u001b[39mimport\u001b[39;00m col,\u001b[39msum\u001b[39m \u001b[39mas\u001b[39;00m spark_sum\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/badrelabbady/Downloads/modele-architec_exoPyspark.ipynb#W4sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m spark \u001b[39m=\u001b[39m SparkSession\u001b[39m.\u001b[39mbuilder \\\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/badrelabbady/Downloads/modele-architec_exoPyspark.ipynb#W4sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m     \u001b[39m.\u001b[39mappName(\u001b[39m\"\u001b[39m\u001b[39mTraitement de données PySpark\u001b[39m\u001b[39m\"\u001b[39m) \\\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/badrelabbady/Downloads/modele-architec_exoPyspark.ipynb#W4sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m     \u001b[39m.\u001b[39mgetOrCreate()\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/badrelabbady/Downloads/modele-architec_exoPyspark.ipynb#W4sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m df \u001b[39m=\u001b[39m spark\u001b[39m.\u001b[39mread\u001b[39m.\u001b[39mcsv(\u001b[39m\"\u001b[39m\u001b[39mventes2.csv\u001b[39m\u001b[39m\"\u001b[39m, header\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/badrelabbady/Downloads/modele-architec_exoPyspark.ipynb#W4sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m df\u001b[39m.\u001b[39mshow()\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/badrelabbady/Downloads/modele-architec_exoPyspark.ipynb#W4sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m df \u001b[39m=\u001b[39m df\u001b[39m.\u001b[39mwithColumn(\u001b[39m\"\u001b[39m\u001b[39mChiffre_d_affaires\u001b[39m\u001b[39m\"\u001b[39m, col(\u001b[39m\"\u001b[39m\u001b[39mQuantité\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39m*\u001b[39m col(\u001b[39m\"\u001b[39m\u001b[39mPrix_unitaire\u001b[39m\u001b[39m\"\u001b[39m))\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pyspark/sql/readwriter.py:740\u001b[0m, in \u001b[0;36mDataFrameReader.csv\u001b[0;34m(self, path, schema, sep, encoding, quote, escape, comment, header, inferSchema, ignoreLeadingWhiteSpace, ignoreTrailingWhiteSpace, nullValue, nanValue, positiveInf, negativeInf, dateFormat, timestampFormat, maxColumns, maxCharsPerColumn, maxMalformedLogPerPartition, mode, columnNameOfCorruptRecord, multiLine, charToEscapeQuoteEscaping, samplingRatio, enforceSchema, emptyValue, locale, lineSep, pathGlobFilter, recursiveFileLookup, modifiedBefore, modifiedAfter, unescapedQuoteHandling)\u001b[0m\n\u001b[1;32m    738\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mtype\u001b[39m(path) \u001b[39m==\u001b[39m \u001b[39mlist\u001b[39m:\n\u001b[1;32m    739\u001b[0m     \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_spark\u001b[39m.\u001b[39m_sc\u001b[39m.\u001b[39m_jvm \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m--> 740\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_df(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jreader\u001b[39m.\u001b[39mcsv(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_spark\u001b[39m.\u001b[39m_sc\u001b[39m.\u001b[39m_jvm\u001b[39m.\u001b[39mPythonUtils\u001b[39m.\u001b[39mtoSeq(path)))\n\u001b[1;32m    741\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(path, RDD):\n\u001b[1;32m    743\u001b[0m     \u001b[39mdef\u001b[39;00m \u001b[39mfunc\u001b[39m(iterator):\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[39m=\u001b[39m proto\u001b[39m.\u001b[39mCALL_COMMAND_NAME \u001b[39m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcommand_header \u001b[39m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[39m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[39m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgateway_client\u001b[39m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[39m=\u001b[39m get_return_value(\n\u001b[1;32m   1323\u001b[0m     answer, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgateway_client, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtarget_id, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mname)\n\u001b[1;32m   1325\u001b[0m \u001b[39mfor\u001b[39;00m temp_arg \u001b[39min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(temp_arg, \u001b[39m\"\u001b[39m\u001b[39m_detach\u001b[39m\u001b[39m\"\u001b[39m):\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pyspark/errors/exceptions/captured.py:185\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    181\u001b[0m converted \u001b[39m=\u001b[39m convert_exception(e\u001b[39m.\u001b[39mjava_exception)\n\u001b[1;32m    182\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    183\u001b[0m     \u001b[39m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    184\u001b[0m     \u001b[39m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 185\u001b[0m     \u001b[39mraise\u001b[39;00m converted \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    186\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    187\u001b[0m     \u001b[39mraise\u001b[39;00m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: [PATH_NOT_FOUND] Path does not exist: file:/Users/badrelabbady/Downloads/ventes2.csv."
     ]
    }
   ],
   "source": [
    "#exercice 1 \n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col,sum as spark_sum\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Traitement de données PySpark\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "df = spark.read.csv(\"ventes2.csv\", header=True)\n",
    "\n",
    "df.show()\n",
    "\n",
    "df = df.withColumn(\"Chiffre_d_affaires\", col(\"Quantité\") * col(\"Prix_unitaire\"))\n",
    "df.show()\n",
    "\n",
    "chiffre_d_affaires_total = df.agg(spark_sum(\"Chiffre_d_affaires\").alias(\"Chiffre_d_affaires_total\")).collect()[0][\"Chiffre_d_affaires_total\"]\n",
    "print(f\"Chiffre d'affaires total : {chiffre_d_affaires_total}\")\n",
    "\n",
    "produit_plus_vendu = df.groupBy(\"Produit\").agg(spark_sum(\"Quantité\").alias(\"Total_Quantité\")) \\\n",
    "    .orderBy(col(\"Total_Quantité\").desc()) \\\n",
    "    .first()\n",
    "\n",
    "print(f\"Produit le plus vendu : {produit_plus_vendu['Produit']} avec {produit_plus_vendu['Total_Quantité']} unités vendues.\")\n",
    "\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#exercice 2 \n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import avg, col\n",
    "\n",
    "spark = SparkSession.builder.appName(\"Analyse Utilisateurs\").getOrCreate()\n",
    "\n",
    "fichier_json = \"utilisateurs.json\"\n",
    "df = spark.read.json(fichier_json)\n",
    "\n",
    "df_cleaned = df.filter(df.nom.isNotNull() & df.âge.isNotNull() & df.ville.isNotNull())\n",
    "\n",
    "age_moyen = df_cleaned.select(avg(\"âge\").alias(\"Âge Moyen\")).first()[\"Âge Moyen\"]\n",
    "\n",
    "utilisateurs_par_ville = df_cleaned.groupBy(\"ville\").count()\n",
    "\n",
    "plus_jeune = df_cleaned.orderBy(\"âge\").first()\n",
    "\n",
    "print(f\"Âge moyen : {age_moyen:.1f} ans\")\n",
    "print(\"Nombre d'utilisateurs par ville :\")\n",
    "utilisateurs_par_ville.show()\n",
    "print(f\"Le plus jeune utilisateur est {plus_jeune['nom']} ({plus_jeune['âge']} ans).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---+---------+------+\n",
      "|   Nom|Age|    Ville|Revenu|\n",
      "+------+---+---------+------+\n",
      "| Alice| 25|    Paris| 50000|\n",
      "|   Bob| 30|     Lyon| 40000|\n",
      "|Claire| 35|Marseille| 35000|\n",
      "| David| 40| Inconnue| 45000|\n",
      "+------+---+---------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#exercice 3 \n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Initialiser SparkSession\n",
    "spark = SparkSession.builder.appName(\"Nettoyage de données\").getOrCreate()\n",
    "\n",
    "# Données\n",
    "data = [(\"Alice\", 25, \"Paris\", 50000),\n",
    "        (\"Bob\", None, \"Lyon\", 40000),\n",
    "        (\"Claire\", 35, \"Marseille\", 35000),\n",
    "        (\"David\", 40, None, 45000),\n",
    "        (\"Emma\", 22, \"Lyon\", None)]\n",
    "columns = [\"Nom\", \"Age\", \"Ville\", \"Revenu\"]  # Renommé Âge en Age\n",
    "\n",
    "# Créer le DataFrame\n",
    "df = spark.createDataFrame(data, columns)\n",
    "\n",
    "# Calcul de la moyenne d'âge\n",
    "age_moyen = df.selectExpr(\"avg(Age) as age_moyen\").collect()[0][0]\n",
    "\n",
    "# Remplir les valeurs manquantes\n",
    "df = df.fillna({\"Age\": age_moyen, \"Ville\": \"Inconnue\"})\n",
    "df = df.na.drop(subset=[\"Revenu\"])\n",
    "\n",
    "# Afficher le DataFrame nettoyé\n",
    "df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------+----+\n",
      "|   Produit|   Catégorie|Prix|\n",
      "+----------+------------+----+\n",
      "|Ordinateur|Électronique| 800|\n",
      "|     Table|      Bureau| 150|\n",
      "|Imprimante|Électronique| 120|\n",
      "+----------+------------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#exercice 4 \n",
    "\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "data = [(\"Ordinateur\", \"Électronique\", 800),\n",
    "        (\"Clavier\", \"Électronique\", 20),\n",
    "        (\"Souris\", \"Électronique\", 15),\n",
    "        (\"Table\", \"Bureau\", 150),\n",
    "        (\"Chaise\", \"Bureau\", 80),\n",
    "        (\"Imprimante\", \"Électronique\", 120)]\n",
    "columns = [\"Produit\", \"Catégorie\", \"Prix\"]\n",
    "\n",
    "df = spark.createDataFrame(data, columns)\n",
    "\n",
    "# Trouver les 3 produits les plus chers\n",
    "produits_plus_chers = df.orderBy(col(\"Prix\").desc()).limit(3)\n",
    "\n",
    "produits_plus_chers.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------------+\n",
      "|Client|sum(Montant)|\n",
      "+------+------------+\n",
      "|  Emma|         350|\n",
      "| Alice|         250|\n",
      "| David|         250|\n",
      "|   Bob|         200|\n",
      "+------+------------+\n",
      "\n",
      "Client ayant dépensé le plus : Emma (350 €)\n"
     ]
    }
   ],
   "source": [
    "#Exercice 5\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Initialiser SparkSession\n",
    "spark = SparkSession.builder.appName(\"Analyse des transactions\").getOrCreate()\n",
    "\n",
    "# Données\n",
    "data = [(\"Alice\", \"2024-01-01\", 150),\n",
    "        (\"Bob\", \"2024-01-02\", 200),\n",
    "        (\"Alice\", \"2024-01-03\", 100),\n",
    "        (\"Emma\", \"2024-01-04\", 300),\n",
    "        (\"David\", \"2024-01-05\", 250),\n",
    "        (\"Emma\", \"2024-01-06\", 50)]\n",
    "columns = [\"Client\", \"Date\", \"Montant\"]\n",
    "\n",
    "# Créer le DataFrame\n",
    "df = spark.createDataFrame(data, columns)\n",
    "\n",
    "# Calcul des dépenses totales\n",
    "depenses_totales = df.groupBy(\"Client\").sum(\"Montant\").orderBy(col(\"sum(Montant)\").desc())\n",
    "\n",
    "# Client ayant dépensé le plus\n",
    "meilleur_client = depenses_totales.first()\n",
    "\n",
    "depenses_totales.show()\n",
    "print(f\"Client ayant dépensé le plus : {meilleur_client['Client']} ({meilleur_client['sum(Montant)']} €)\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
