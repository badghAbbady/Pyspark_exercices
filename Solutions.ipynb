{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/12/18 11:55:27 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "24/12/18 11:55:28 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "24/12/18 11:55:31 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: Date, Produit, Quantité, Prix_unitaire, \n",
      " Schema: Date, Produit, Quantité, Prix_unitaire, _c4\n",
      "Expected: _c4 but found: \n",
      "CSV file: file:///Users/badrelabbady/Desktop/exo1/ventes2.csv\n",
      "24/12/18 11:55:31 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: Date, Produit, Quantité, Prix_unitaire, \n",
      " Schema: Date, Produit, Quantité, Prix_unitaire, _c4\n",
      "Expected: _c4 but found: \n",
      "CSV file: file:///Users/badrelabbady/Desktop/exo1/ventes2.csv\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+--------+-------------+----+\n",
      "|      Date|   Produit|Quantité|Prix_unitaire| _c4|\n",
      "+----------+----------+--------+-------------+----+\n",
      "|2024-01-01|Ordinateur|       2|          800|NULL|\n",
      "|2024-01-01|   Clavier|       5|           20|NULL|\n",
      "|2024-01-02|    Souris|      10|           15|NULL|\n",
      "|2024-01-02|Ordinateur|       1|          800|NULL|\n",
      "|2024-01-03|   Clavier|       3|           20|NULL|\n",
      "+----------+----------+--------+-------------+----+\n",
      "\n",
      "+----------+----------+--------+-------------+----+------------------+\n",
      "|      Date|   Produit|Quantité|Prix_unitaire| _c4|Chiffre_d_affaires|\n",
      "+----------+----------+--------+-------------+----+------------------+\n",
      "|2024-01-01|Ordinateur|       2|          800|NULL|            1600.0|\n",
      "|2024-01-01|   Clavier|       5|           20|NULL|             100.0|\n",
      "|2024-01-02|    Souris|      10|           15|NULL|             150.0|\n",
      "|2024-01-02|Ordinateur|       1|          800|NULL|             800.0|\n",
      "|2024-01-03|   Clavier|       3|           20|NULL|              60.0|\n",
      "+----------+----------+--------+-------------+----+------------------+\n",
      "\n",
      "Chiffre d'affaires total : 2710.0\n",
      "Produit le plus vendu : Souris avec 10.0 unités vendues.\n"
     ]
    }
   ],
   "source": [
    "#exercice 1 \n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col,sum as spark_sum\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Traitement de données PySpark\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "df = spark.read.csv(\"ventes2.csv\", header=True)\n",
    "\n",
    "df.show()\n",
    "\n",
    "df = df.withColumn(\"Chiffre_d_affaires\", col(\"Quantité\") * col(\"Prix_unitaire\"))\n",
    "df.show()\n",
    "\n",
    "chiffre_d_affaires_total = df.agg(spark_sum(\"Chiffre_d_affaires\").alias(\"Chiffre_d_affaires_total\")).collect()[0][\"Chiffre_d_affaires_total\"]\n",
    "print(f\"Chiffre d'affaires total : {chiffre_d_affaires_total}\")\n",
    "\n",
    "produit_plus_vendu = df.groupBy(\"Produit\").agg(spark_sum(\"Quantité\").alias(\"Total_Quantité\")) \\\n",
    "    .orderBy(col(\"Total_Quantité\").desc()) \\\n",
    "    .first()\n",
    "\n",
    "print(f\"Produit le plus vendu : {produit_plus_vendu['Produit']} avec {produit_plus_vendu['Total_Quantité']} unités vendues.\")\n",
    "\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/12/18 11:55:35 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Âge moyen : 30.4 ans\n",
      "Nombre d'utilisateurs par ville :\n",
      "+---------+-----+\n",
      "|    ville|count|\n",
      "+---------+-----+\n",
      "|Marseille|    1|\n",
      "|    Paris|    2|\n",
      "|     Lyon|    2|\n",
      "+---------+-----+\n",
      "\n",
      "Le plus jeune utilisateur est Emma (22 ans).\n"
     ]
    }
   ],
   "source": [
    "#exercice 2 \n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import avg, col\n",
    "\n",
    "spark = SparkSession.builder.appName(\"Analyse Utilisateurs\").getOrCreate()\n",
    "\n",
    "fichier_json = \"utilisateurs.json\"\n",
    "df = spark.read.json(fichier_json)\n",
    "\n",
    "df_cleaned = df.filter(df.nom.isNotNull() & df.âge.isNotNull() & df.ville.isNotNull())\n",
    "\n",
    "age_moyen = df_cleaned.select(avg(\"âge\").alias(\"Âge Moyen\")).first()[\"Âge Moyen\"]\n",
    "\n",
    "utilisateurs_par_ville = df_cleaned.groupBy(\"ville\").count()\n",
    "\n",
    "plus_jeune = df_cleaned.orderBy(\"âge\").first()\n",
    "\n",
    "print(f\"Âge moyen : {age_moyen:.1f} ans\")\n",
    "print(\"Nombre d'utilisateurs par ville :\")\n",
    "utilisateurs_par_ville.show()\n",
    "print(f\"Le plus jeune utilisateur est {plus_jeune['nom']} ({plus_jeune['âge']} ans).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---+---------+------+\n",
      "|   Nom|Age|    Ville|Revenu|\n",
      "+------+---+---------+------+\n",
      "| Alice| 25|    Paris| 50000|\n",
      "|   Bob| 30|     Lyon| 40000|\n",
      "|Claire| 35|Marseille| 35000|\n",
      "| David| 40| Inconnue| 45000|\n",
      "+------+---+---------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#exercice 3 \n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Initialiser SparkSession\n",
    "spark = SparkSession.builder.appName(\"Nettoyage de données\").getOrCreate()\n",
    "\n",
    "# Données\n",
    "data = [(\"Alice\", 25, \"Paris\", 50000),\n",
    "        (\"Bob\", None, \"Lyon\", 40000),\n",
    "        (\"Claire\", 35, \"Marseille\", 35000),\n",
    "        (\"David\", 40, None, 45000),\n",
    "        (\"Emma\", 22, \"Lyon\", None)]\n",
    "columns = [\"Nom\", \"Age\", \"Ville\", \"Revenu\"]  # Renommé Âge en Age\n",
    "\n",
    "# Créer le DataFrame\n",
    "df = spark.createDataFrame(data, columns)\n",
    "\n",
    "# Calcul de la moyenne d'âge\n",
    "age_moyen = df.selectExpr(\"avg(Age) as age_moyen\").collect()[0][0]\n",
    "\n",
    "# Remplir les valeurs manquantes\n",
    "df = df.fillna({\"Age\": age_moyen, \"Ville\": \"Inconnue\"})\n",
    "df = df.na.drop(subset=[\"Revenu\"])\n",
    "\n",
    "# Afficher le DataFrame nettoyé\n",
    "df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------+----+\n",
      "|   Produit|   Catégorie|Prix|\n",
      "+----------+------------+----+\n",
      "|Ordinateur|Électronique| 800|\n",
      "|     Table|      Bureau| 150|\n",
      "|Imprimante|Électronique| 120|\n",
      "+----------+------------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#exercice 4 \n",
    "\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "data = [(\"Ordinateur\", \"Électronique\", 800),\n",
    "        (\"Clavier\", \"Électronique\", 20),\n",
    "        (\"Souris\", \"Électronique\", 15),\n",
    "        (\"Table\", \"Bureau\", 150),\n",
    "        (\"Chaise\", \"Bureau\", 80),\n",
    "        (\"Imprimante\", \"Électronique\", 120)]\n",
    "columns = [\"Produit\", \"Catégorie\", \"Prix\"]\n",
    "\n",
    "df = spark.createDataFrame(data, columns)\n",
    "\n",
    "# Trouver les 3 produits les plus chers\n",
    "produits_plus_chers = df.orderBy(col(\"Prix\").desc()).limit(3)\n",
    "\n",
    "produits_plus_chers.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------------+\n",
      "|Client|sum(Montant)|\n",
      "+------+------------+\n",
      "|  Emma|         350|\n",
      "| Alice|         250|\n",
      "| David|         250|\n",
      "|   Bob|         200|\n",
      "+------+------------+\n",
      "\n",
      "Client ayant dépensé le plus : Emma (350 €)\n"
     ]
    }
   ],
   "source": [
    "#Exercice 5\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Initialiser SparkSession\n",
    "spark = SparkSession.builder.appName(\"Analyse des transactions\").getOrCreate()\n",
    "\n",
    "# Données\n",
    "data = [(\"Alice\", \"2024-01-01\", 150),\n",
    "        (\"Bob\", \"2024-01-02\", 200),\n",
    "        (\"Alice\", \"2024-01-03\", 100),\n",
    "        (\"Emma\", \"2024-01-04\", 300),\n",
    "        (\"David\", \"2024-01-05\", 250),\n",
    "        (\"Emma\", \"2024-01-06\", 50)]\n",
    "columns = [\"Client\", \"Date\", \"Montant\"]\n",
    "\n",
    "# Créer le DataFrame\n",
    "df = spark.createDataFrame(data, columns)\n",
    "\n",
    "# Calcul des dépenses totales\n",
    "depenses_totales = df.groupBy(\"Client\").sum(\"Montant\").orderBy(col(\"sum(Montant)\").desc())\n",
    "\n",
    "# Client ayant dépensé le plus\n",
    "meilleur_client = depenses_totales.first()\n",
    "\n",
    "depenses_totales.show()\n",
    "print(f\"Client ayant dépensé le plus : {meilleur_client['Client']} ({meilleur_client['sum(Montant)']} €)\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
